Interview 2023:
1. Please tell me about yourself?
      My name is Simon, I have over 9 years in IT with 5years working as a  
      DevOps as DevOps/Cloud/SR/Platform/Kubernetes/Infrastructure Engineer. 
      I have a strong drive for automation, containers and cloud computing.  

2. What kind project are you managing?
     Thanks for the question, in my environment I have managed couple of projects  aimed 
     at achieving:
        Develops, Tests, Builds, qualification, backup  
        deploys & monitors applications 
       Applications are the output from Softwares development 
     including: 
       1. Provisioning and configuring Infrastructures and servers in AWS using  
          terraform, ansible and Jenkins
          pipeline

       2. Building and maintaining automation pipelines with jenkins for application;
          build, test and upload artifacts into nexus using Jenkins CI job
          tools = Jenkins with Git/Github, maven, sonarqube, nexus, Notification 

      3.  Building automation pipelines with jenkins for application deployment
          using Jenkins with tomcat or Jenkins with docker, dockerhub and Kubernetes 

      4.  Runinng Jenkins continuous monitoring jobs using:
          Jenkins with Kubernetes, prometheus and grafana
          Jenkins with Kubernetes, elasticsearch, filebeat and kibana

     5.  My environment is heavy on aws cloud.  
         Cloud
    6.   Security is inherent in our environment. CS = continuous security 

    7. I have written and maintained ansible-playbooks used by our Systems admins  
       for: patching, userMGT, fileMgt, packageMGT. I am  the escalation point if  
        issues arise while running the ansible-playbooks   

  8. In my environment, I manage Java based project.

web: = 54.221.153.174 key29.pem ubuntu
ansible: 3.86.243.51 key29.pem" ec2-user
  sudo useradd ansible  
  sudo yum -y install python3-pip
  sudo apt -y install python3-pip
  pip3 install ansible --user
  
 #create ansible folder under /etc/
 sudo mkdir /etc/ansible
 sudo chown -R ansible:ansible /etc/ansible
 create ansible .cfig file under /etc/ansible
 vi /etc/ansible/ansible.cfg
 https://github.com/ansible/ansible/blob/stable-2.9/examples/ansible.cfg
 #To change ip hosts 
 vi /etc/ansible/hosts
 
  pip3 install boto3 botocore --user
  pip3 install --upgrade requests --user
appserver: 
  44.204.71.5 class30key.pem" ec2-user
jenkins: 3.223.191.159 class30key.pem" ec2-user


terraform: 54.82.118.56  key29.pem" ubuntu
 sh tf.sh 
 #!/bin/bash
# install terraform in ubuntu server
sudo hostnamectl set-hostname tf
sudo apt install wget unzip -y
wget https://releases.hashicorp.com/terraform/0.12.26/terraform_0.12.26_linux_amd64.zip
sudo unzip terraform_0.12.26_linux_amd64.zip -d /usr/local/bin/
Export terraform binary path temporally
export PATH=$PATH:/usr/local/bin
#Add path permanently for current user.By Exporting path in 
#.bashrc file at end of file.
 echo  "export PATH=$PATH:/usr/local/bin" >> ~/.bashrc
# Source .bashrc to reflect for current session
 source ~/.bashrc
 terraform -version
==================================================


OPERATORS:
  nginx-ingress --- ALB/ELB    
  serviceMesh and Istio 
  Metric-server

10 micro service enterprise applications
11 micro-services per application  
   110 --- 10 pods  = 1100 pods/containers
   
   jenkins initial password: 2106c7ee1cec4fd9a438781ffbadbfd0
   
   MD - Dec 7, 2022
   Running-notes
   
   Patience

Dozie
Position  = Azure DevOps/ES Engineer   
====================================
1. How can you deploy 5m files to 500 servers using IaaC  
       Any Cloud = Terraform/ansible/
       AWS - CloudFormation & System Manager  
       AZURE - Resource manager 
       GCP - Google Cloud Resource
  dbservers 
  file
  In my env, I used ansible or terraform for file migration. 
  We can create an archive for the file, then we unzip the file with the help of ansible, we deploy to the target host.
  I would create ansible-playbook to migrate the file.

  ansible dbservers -m copy -a "src=tesla.zip dest=/opt" -b 
  
- hosts: localhost  
  tasks:
  - name: install package
    package:
      name: ['zip', 'tree', 'nano', 'unzip', 'wget']
      state: latest
  - name: copy file 
    shell: cp *  tesla/ 
    ignore_errors: true
  - name: create archive 
    shell: zip tesla tesla.zip 
    ignore_errors: true
- hosts: web  
  tasks:
  - name: copy files 
    copy: 
      src: tesla.zip 
      dest: /opt/ 

b) they went straight to the role - no time for morning devotion 
c) What are your experiences with elasticsearch?
      
https://www.elastic.co/elasticsearch
Elasticsearch is the world’s leading free and open search and analytics solution. With an emphasis on speed, scale, and relevance it's transforming how the world uses data.

Elasticsearch is a distributed, RESTful search and analytics engine capable of addressing a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fine‑tuned relevancy, and powerful analytics that scale with ease.
-In our env, we use elasticsearch filebeat and kibana for data analytic and log management. Elasticsearch come with DB, and I deploy it in my env as statefullSet.
Elasticsearch would help to analyse how the app is behaving.Elasticsearch would scrap the logs and does log management, analytic, indexing and with the help
of kibana we can visualize what is happening.
  amazon.com 
  google.com = i9 laptop  

d) How would you deploy 5m files to 500 servers using IAAC?
e) How would these files get to ElasticSearch

f) How does your company access elasticsearch on the browser?
In our env, we're deploying elasticsearch as k8s object, and we're using kibana to visualize what is happening inside.

g) How do you secure your k8s cluster? 
     namespaces: I can restrict access to certain namespaces, example:Developer do not have access to production namespace.
     RBAC [  ]:
        users / groups / ServiceAccounts
        Role/RoleBinding  
        ClusterRole / ClusterRoleBinding  
     IAM 
     resourceQuota to determine how much resources can be consume by a particular namespace.

h) What files do you see in your helm chart?
      -(file)= values.yml[deploy, pvc, ClusterIP, SA, R/RB, CR,CRB, HPA, ingress ] / 
      -(directory)= template/deploy.yml, hpa.yml, sa.yml, rbac.yml, ingress.yml, service.yml 
      
I) what commands exist in your dockerfile?
   	-FROM used to choose my based image which should have my app dependencies
   	-MAINTAINER who have have access to the docker file should know it maintain by us
   	-ADD use to copy file from docker host server to container while creating the image; use also to copy file from external URL.
   	-COPY copy file from docker host server to the container while creating the image
   	-RUN use to execute command while creating the docker image; it is important that we minimize our run instructions so that we can create light weigh images
   	-EXPOSE use to state the port that our container would listening on 
   	-USER tomcat by default docker run on root; if we want to change the user that would manage our containerization we should state with USER.   
   	-VOLUME if we're creating docker file to deploy statefull app we need to create a volume to determine where our data would be mountain in the container
   	-WORDIR  /usr/local/tomcat is where the entire task would be running
   	-ENV pass env variables
   	-LABELS
   	-CMD executed while creating and starting container
   	-ARG
   	-ENTRYPOINT executed while creating and starting container
      
   j) How do you secure your k8s cluster and network?
   When we deploy app, we restrict who can have access using namespace.
   
   i) What Azure/aws tool do you secure your network with?
   To secure our network in aws, we have vpc, subnet concept[private and public], network access control list[nacl], security group waf, iam
   
   k) walk me through your terraform steps in creating a resource.
   In our env provisionning any resource with terraform, we have modules in our env. For example if I have to deploy vpc in azure or aws,
   we have a module which is going to deploy ec2 instance; I have to check the module for any information and in our env it is done in our variable.tf file.
   I have module for most of my deployment and make sure that terraform has the permission to manage and create resources so the right IAM permission is assign, then
   I go through my terraform work flow, I would run terraform init to initialize the backend and to download all the provider plugins, terraform format to organize
   terraform and tf file, terraform validate to make sure the syntax are ok, terraform plan to see the resources that would be created, terraform apply to provisione
   the resources.
   
   L) Have you used azure code pipeline?
   I have not used azure code pipeline, but in aws I use similar resource aws code pipeline 
   
   M) How do you write a groovy custom line in your declarative pipeline in jenkins?
   In groovy, I generally use a jinja bracket.{}
   
   N) Have you used multi stage docker file before?
   

 docker exec myapp pwd 

       azure  / aws /  private  
IaaC   RM      CF      vagrant  
======================================================
Dec 7, 2022
===============
        Tesla 
Taboh
1. Tell us about yourself 

2. Which technologies have you used in your environment
In my env, I used AWS as our Cloud Provider and we're heavy on AWS. I use terraform to provisione resources in AWS from vpc, S3 buckect, EKS cluster, EC2 instances
among othres. I use ansible to configure our env; we're heavy on infrastructure as Code. We use Jenkins for CI/CD in our env. We use Git for versioning , and Github
as our SCM. We use Maven to build our java based project. We use Sonarqube for code review and code quality analysis. We use Nexus as our artifactory. We use
Docker for containerization and k8s for container orchestration. We use prometheus and grafana for application performance monitoring including New Relic. We use 
Elasticsearch Filebeat and kibana for log management and data analysis.

3. Explain how you apply terraform in your env'
In my env, we're heavy on Iac, we use terraform as our major Iac tool to provisione different resources in AWS Cloud which include create VPC and all its components
which include subnet, route table, internet gateway. I also use terraform to deploy different env as dev env, stage env, pod env include all the resources that 
are needed like EC2 instance, Elastic load balancer. When applying terraform we use of modules because we do not only provisione resources for one env, we provisione
resources for multiples env. We have modules to create vpc, ec2 instances, for elastic ip, virtualy for everything we need in our env. We ensure that we backup 
our state file in s3 bucket. We log our state file using dynamoDB because we do not want to have a situation where two Engineers are use the state file at a same 
type. Our env is highly secure with terraform. My experience with terraform also include importing resources with terraform that was not created under terraform
management because in our env we want to make sure that everything we have is manage via IaC. For that effect, I run terraform import to import resources that was 
either manually created from the consol or via CLI so that terraform can manage those resources.

4. What problems have you encountered applying terraform 
  I had a problem where I was ask to provisioning infrastructure with terraform but i had permission issue, and it was not permiting me to provisione certain resources
  My accesskey/secretAccess key was [EC2ReadAccess] that means I could not provisione infrastructure. I imediately spoke to my team lead who granted me the access
  create that resource. I had syntax issue when I run terraform init or terraform validate where the .tf cannot be validate because I had to make some changes 
  in the tf file. I had syntax problem because variables were pass differently in the terraform script I wanted to use. I can modify the tf file match the change
  in the latest version or I can fur terraform with the version which is compactible with my tf file. I use terraform to provisione resources in the dev en, stage 
  and production and if we harcorde values, it would be difficult to use it in different env, and that would challenge the purpose of automation that our env
  is heavy on. To avoid hard coding, we use variables, so we create variable file so that we can use one module to provisione resoureces in different env.
  I had problem with region secification, when creating ec2 instance the iam-id [amazon machine image] was different] from the resources and wanted to provisione.
  To fix the problem, I had to make sure that iam is passed as variables. When we introduce terraform in our env, we have some of our Jr Engineers passing their AWS
  credential(accesskey) as part of their terraform file that was not a good security measure, and we have to fix the problem imediately. Two years ago, we had a situation
  where we were try to run terraform apply and the infrastructure was not alloying us to provisione, we had breakage, and what we realize was that multiple 
  Engineers were running terraform apply at the same time, they were try to provisione resources at the same time and that was a serious problem. 
  We then introduce S3 backend and dynamoDB table to log our terraform file.
  
   permissions/syntax/plugins/wrong version/ 
   hardcoded values /regions-specification/attaching keys
   variables: maps/lists/interger/strings/

   Taboh = accessKey/secretAccessKey[EKS EC2READACCESS]

AMI-id ubuntu:
             us-east-1   us-east-2  us-west-1  

terraform_1.4.0-alpha20221109
terraform_1.3.6
terraform_1.3.5
terraform_1.3.4
terraform_1.2.4
terraform_1.2.3
terraform_0.1.9

5. Explain a project you did which you are proud of??
My name is Cecile, and thank you again for this question. As a DevOps Engineer, I personaly take pride for what I do and almost all the project I done I am really
excited about them, but if I was about to talk about one of this project I remember that I was assign a ticket. We had a meeting and our main discussion 
was how to automate automation. We had to provisione an amazon EKS cluster that was my task. We have different options to provisione the cluster,one of the ways to 
provisione from the console or commande line, but my main vue was like how can I use terraform to provisione this cluster. I did that project on how to use
terraform to provisione the cluster. I came back and I told my team manager that I found a different method we could use to provisioning EKS. We can use 
IaC which is terraform and it would provisioning EKS and all its component because EKS come with vpc, private and public subnet, loadBalancer, auto scaling
group, IAM roles etc. I wrote a terraform file and we try to prove the concept in it to see if it is going to work, we ran it in work and it was well
everybody was so exited and since then EKS is what we use to provisione our cluster in any env. We want to provisione EKS in dev env we just use the same EKS
terraform code. That's the project I did I'm very proud of, and I also remember introducing to my team this integration Jenkins-terraform pipeline
because in the past it was like terraform is already automation so no point for us to be provisione terraform with jenkins. It is good to have one dasboard
wherewe can visualize everything and we create our terraform pipeline in jenkins and from there, we are able to provisione resources. This are some project
I can say I am very excited about them because it took automation in our env in a next level and eveybody was so excited and I even receive an award, and I 
see that as one of my greatest achievement. I was the DevOps Engineer of the quater. 

b. What are you proud of in your env?
When I started working, we have some of our jenkins jobs that was intended to deploy app, and one thing that I noticed was that, we have a job that after jenkins
does a build, docker need to containerize and everything was running in jenkins server so docker was install on jenkins, everything was install on jenkins.
I advice my team that it was not a best approach to apply jenkins because we're applying jenkins without making provision for disaster recovery. Our infrastructure
was not for tolerant. I had to present to them how we can apply and carry the same job using Jenkins master slave architecture.
Example: Jenkins - terraform integration:
-Access Jenkins server on the browser
-Manage jenkins
-manage Nodes and cloud
-add new Node (terraform) permanent agent create
-description (terraform)
-Number of executors (4)
-remote root directory (mandatory :/home/ubuntu/jk)
-Labels (terraformnode)
-Launch method(via SSH): HostName(IP address); credential(add credential:Provider Jenkins=SSH Username with private key,ID=terraform-cred, description=terraform-cred,
Username=ubuntu, Private key Enter directly add key copy and paste, key verification strategy=Manually trusted key verification strategy, save).
Install Java for the agent to be online.

add Ansible agent
-Copy configuration from terraform and modify (manage Ansible as ansible user)
-Labels (ansiblenode)
-Hostname=ansible IP adress; Username=ansible; Launch method; credential (Username and password; username=ansible; password=admin123; ID=ansible-cred;
description=ansible-cred) save. Go to ansible server and switch to ansible assign password to ansible= sudo passwd ansible=admin123; install java;
sudo vi /etc/ssh/sshd_config modify PasswodrAuthentication from no to yes in ansible. then sudo systemctl restart sshd.
pipeline{
   agent{
      label 'terraformnode'
   }
   stages{
      stage('clonecode'){
         steps{
            git"https://github.com/Cemilie/jenkins-ansible-dynamic-inv"
         }
      }
      stage('createInfra'){
          steps{
             sh "terraform init"
             sh "terraform apply fmt"
             sh "terraform validate"
             sh "terraform show"
             sh "terraform apply --auto-approve"
           }
        }
     }
 }

create new pipeline for terraform-project
-Go to AWS terraform, then action, security, modify IAM role, attach role with administratorAccess and go back to run build.
6. What are you your greatest achievements as a DevOps Engineer

 maven / sonarqube /nexus

Jenkins - kubernetes / 
Jenkins - ansible / 
Jenkins - docker / 
Jenkins - terraform /

Jenkins master slave architecture:



======================================================================
Dec 8, 2022                                                       ====:
======================================================================
Explain a project you currently manage  

Explain a problem you faced in your project recently
   deprecated helm chart
       custom helm charts  

       3rd party helm charts
   
    helm install myapp app/app
        values.yml  template/deploy.yml service.yml ingress.yml hpa.yml sa.yml rbac.yml 
        helm repo update   
   kubernetes version = 1.25 / 1.10 / 1.15 1.20[docker]
   deprecated apiVersion vs kubectlVersion kubeletVersion  
   docker deprecated
      nodes[kubelet/kubeproxy/container-runtime=container-D/]
   expired token []
   Authentication [kube/config, rbac, ]:
       developer [ Role/RoleBinding ClusterRole/ClusterRoleBinding] 
       Role
   image=Tag/wrongRepo
   scaling issues
   Users could not access the application
   insufficient resources on nodes
   syntax
   Pod crashing
   service not routing traffic to application
kubectl apply -f app.yml  
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-resource-1
spec:
  ingressClassName: nginx
  rules:
  - host: springboot.example.com
    http:
      paths:
      # Default Path(/)
      - backend:
          serviceName: springboot
          servicePort: 80
      - path: /java-web-app
        backend:
          serviceName: javawebapp
          servicePort: 80 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create","delete", "describe"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: superadmin
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["*"]
  verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# You can specify more than one "subject"
- kind: User
  name: john # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: managers # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # "roleRef" specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: superadmin # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows "jane" to read pods in the "default" namespace.
# You need to already have a Role named "pod-reader" in that namespace.
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# You can specify more than one "subject"
- kind: User
  name: jane # "name" is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # "roleRef" specifies the binding to a Role / ClusterRole
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io




   
